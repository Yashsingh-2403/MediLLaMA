Hosting this project is currently not possible for me. I would like to record the Interaction and upload here the Link soon.

Using the MedDialog (English) dataset, which contains thousands of anonymized medical consultations, the model was trained using Low-Rank Adaptation (LoRA) and 4-bit quantization for memory-efficient fine-tuning. The Hugging Face transformers, datasets, and peft libraries were used for training, tokenization, and parameter-efficient optimization.

